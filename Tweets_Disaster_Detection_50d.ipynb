{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dynamic-there",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:16:03.886350Z",
     "start_time": "2021-08-12T04:16:02.871483Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-thesis",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "funky-cable",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:24:30.502687Z",
     "start_time": "2021-08-12T04:24:30.474849Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/Users/chemin/Data_Science/Now_21Spring/advanced_topics/final_project/nlp-getting-started/train.csv\")\n",
    "df_test =  pd.read_csv(\"/Users/chemin/Data_Science/Now_21Spring/advanced_topics/final_project/nlp-getting-started/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "accessible-cartridge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:24:30.806507Z",
     "start_time": "2021-08-12T04:24:30.797894Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "meaningful-graduation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:24:31.047725Z",
     "start_time": "2021-08-12T04:24:31.044454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7613"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "potential-window",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:24:31.251877Z",
     "start_time": "2021-08-12T04:24:31.246558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-vatican",
   "metadata": {},
   "source": [
    "## Tweets Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "grave-symphony",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:24:31.518038Z",
     "start_time": "2021-08-12T04:24:31.513567Z"
    }
   },
   "outputs": [],
   "source": [
    "class TweetsExample:\n",
    "    \"\"\"\n",
    "    Data wrapper for a single example for classification.\n",
    "\n",
    "    Attributes:\n",
    "        words (List[string]): list of words\n",
    "        label (int): 0 or 1 (0 = negative, 1 = positive)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, words, label):\n",
    "        self.words = words\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.words) + \"; label=\" + repr(self.label)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "basic-patrick",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:24:31.649253Z",
     "start_time": "2021-08-12T04:24:31.645420Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_normalized_words(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Given a string, return a list of words normalized as follows.\n",
    "    Split the string to make words first by using regex compile() function\n",
    "    and string.punctuation + '0-9\\\\r\\\\t\\\\n]' to replace all those char with a space character.\n",
    "    Split on space to get word list.\n",
    "    Ignore words < 3 char long.\n",
    "    Lowercase all words.\n",
    "    \"\"\"\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text)  # delete stuff but leave at least a space to avoid clumping together\n",
    "    words = nopunct.split(\" \")\n",
    "    words = [w for w in words if len(w) > 2]  # ignore a, an, to, at, be, ...\n",
    "    words = [w.lower() for w in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "indoor-clothing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:24:31.814568Z",
     "start_time": "2021-08-12T04:24:31.810738Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_tweets_examples(df) -> List[TweetsExample]:\n",
    "    \"\"\"\n",
    "    Reads tweets examples in the df format; tokenizes and cleans the sentences and forms\n",
    "    TweetsExample.\n",
    "\n",
    "    NOTE: we need to lowercase the data. This is because the GloVe embeddings don't\n",
    "    distinguish case and so can only be used with lowercasing.\n",
    "\n",
    "    :param df: df to read from\n",
    "    :return: a list of TweetsExamples parsed from the df\n",
    "    \"\"\"\n",
    "    exs = []\n",
    "    for i in range(len(df)):\n",
    "        sent = df.iloc[i]['text']\n",
    "        tokenized_cleaned_sent = get_normalized_words(sent)\n",
    "        label  = df.iloc[i]['target']\n",
    "        exs.append(TweetsExample(tokenized_cleaned_sent, label))\n",
    "    return exs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "exempt-cheese",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:24:31.945610Z",
     "start_time": "2021-08-12T04:24:31.941789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = df_train.iloc[0]['text']\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ethical-cabinet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:24:32.103516Z",
     "start_time": "2021-08-12T04:24:32.098912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df_train.iloc[0]['target']\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "equipped-republic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:24:32.463315Z",
     "start_time": "2021-08-12T04:24:32.458879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['our',\n",
       " 'deeds',\n",
       " 'are',\n",
       " 'the',\n",
       " 'reason',\n",
       " 'this',\n",
       " 'earthquake',\n",
       " 'may',\n",
       " 'allah',\n",
       " 'forgive',\n",
       " 'all']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_normalized_words(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-neutral",
   "metadata": {},
   "source": [
    "## Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "elect-helen",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:24:32.803728Z",
     "start_time": "2021-08-12T04:24:32.796713Z"
    }
   },
   "outputs": [],
   "source": [
    "class Indexer(object):\n",
    "    \"\"\"\n",
    "    Bijection between objects and integers starting at 0. Useful for mapping\n",
    "    labels, features, etc. into coordinates of a vector space.\n",
    "\n",
    "    Attributes:\n",
    "        objs_to_ints\n",
    "        ints_to_objs\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.objs_to_ints = {}\n",
    "        self.ints_to_objs = {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str([str(self.get_object(i)) for i in range(0, len(self))])\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.objs_to_ints)\n",
    "\n",
    "    def get_object(self, index):\n",
    "        \"\"\"\n",
    "        :param index: integer index to look up\n",
    "        :return: Returns the object corresponding to the particular index or None if not found\n",
    "        \"\"\"\n",
    "        if (index not in self.ints_to_objs):\n",
    "            return None\n",
    "        else:\n",
    "            return self.ints_to_objs[index]\n",
    "\n",
    "    def contains(self, object):\n",
    "        \"\"\"\n",
    "        :param object: object to look up\n",
    "        :return: Returns True if it is in the Indexer, False otherwise\n",
    "        \"\"\"\n",
    "        return self.index_of(object) != -1\n",
    "\n",
    "    def index_of(self, object):\n",
    "        \"\"\"\n",
    "        :param object: object to look up\n",
    "        :return: Returns -1 if the object isn't present, index otherwise\n",
    "        \"\"\"\n",
    "        if (object not in self.objs_to_ints):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.objs_to_ints[object]\n",
    "\n",
    "    def add_and_get_index(self, object, add=True):\n",
    "        \"\"\"\n",
    "        Adds the object to the index if it isn't present, always returns a nonnegative index\n",
    "        :param object: object to look up or add\n",
    "        :param add: True by default, False if we shouldn't add the object. If False, equivalent to index_of.\n",
    "        :return: The index of the object\n",
    "        \"\"\"\n",
    "        if not add:\n",
    "            return self.index_of(object)\n",
    "        if (object not in self.objs_to_ints):\n",
    "            new_idx = len(self.objs_to_ints)\n",
    "            self.objs_to_ints[object] = new_idx\n",
    "            self.ints_to_objs[new_idx] = object\n",
    "        return self.objs_to_ints[object]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-wrapping",
   "metadata": {},
   "source": [
    "## Pre-trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hawaiian-keyboard",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:24:33.318608Z",
     "start_time": "2021-08-12T04:24:33.314218Z"
    }
   },
   "outputs": [],
   "source": [
    "class WordEmbeddings:\n",
    "    \"\"\"\n",
    "    Wraps an Indexer and a list of 1-D numpy arrays where each position in the list is the vector for the corresponding\n",
    "    word in the indexer. The 0 vector is returned if an unknown word is queried.\n",
    "    \"\"\"\n",
    "    def __init__(self, word_indexer, vectors):\n",
    "        self.word_indexer = word_indexer\n",
    "        self.vectors = vectors\n",
    "\n",
    "    def get_embedding_length(self):\n",
    "        return len(self.vectors[0])\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"\n",
    "        Returns the embedding for a given word\n",
    "        :param word: The word to look up\n",
    "        :return: The UNK vector if the word is not in the Indexer or the vector otherwise\n",
    "        \"\"\"\n",
    "        word_idx = self.word_indexer.index_of(word)\n",
    "        if word_idx != -1:\n",
    "            return self.vectors[word_idx]\n",
    "        else:\n",
    "            return self.vectors[self.word_indexer.index_of(\"UNK\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "polish-isaac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:24:33.451417Z",
     "start_time": "2021-08-12T04:24:33.445812Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_word_embeddings(embeddings_file: str) -> WordEmbeddings:\n",
    "    \"\"\"\n",
    "    Loads the given embeddings (ASCII-formatted) into a WordEmbeddings object. Augments this with an UNK embedding\n",
    "    that is the 0 vector. \n",
    "    :param embeddings_file: path to the file containing embeddings\n",
    "    :return: WordEmbeddings object reflecting the words and their embeddings\n",
    "    \"\"\"\n",
    "    f = open(embeddings_file)\n",
    "    word_indexer = Indexer()\n",
    "    vectors = []\n",
    "    # Make position 0 a PAD token, which can be useful in implementing batching.\n",
    "    word_indexer.add_and_get_index(\"PAD\")\n",
    "    # Make position 1 the UNK token\n",
    "    word_indexer.add_and_get_index(\"UNK\")\n",
    "    for line in f:\n",
    "        if line.strip() != \"\":\n",
    "            space_idx = line.find(' ')\n",
    "            word = line[:space_idx]\n",
    "            numbers = line[space_idx+1:]\n",
    "            float_numbers = [float(number_str) for number_str in numbers.split()]\n",
    "            vector = np.array(float_numbers)\n",
    "            word_indexer.add_and_get_index(word)\n",
    "            # Append the PAD and UNK vectors to start. Have to do this weirdly because we need to read the first line\n",
    "            # of the file to see what the embedding dim is\n",
    "            if len(vectors) == 0:\n",
    "                vectors.append(np.zeros(vector.shape[0]))\n",
    "                vectors.append(np.zeros(vector.shape[0]))\n",
    "            vectors.append(vector)\n",
    "    f.close()\n",
    "    print(\"Read in \" + repr(len(word_indexer)) + \" vectors of size \" + repr(vectors[0].shape[0]))\n",
    "    # Turn vectors into a 2-D numpy array\n",
    "    return WordEmbeddings(word_indexer, np.array(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "sublime-miracle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:36:27.696880Z",
     "start_time": "2021-08-12T04:36:21.940870Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 400002 vectors of size 50\n"
     ]
    }
   ],
   "source": [
    "embeddings_file = '/Users/chemin/Data_Science/Now_21Spring/advanced_topics/final_project/glove.6B.50d.txt'\n",
    "word_embeddings = read_word_embeddings(embeddings_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-crack",
   "metadata": {},
   "source": [
    "## Train/Valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "changed-housing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:36:30.541507Z",
     "start_time": "2021-08-12T04:36:30.533279Z"
    }
   },
   "outputs": [],
   "source": [
    "train = df_train.sample(frac=.8)\n",
    "valid = df_train[~df_train.index.isin(train.index)]\n",
    "train = train.reset_index(drop=True)\n",
    "valid = valid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "judicial-shareware",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:36:40.362179Z",
     "start_time": "2021-08-12T04:36:38.664281Z"
    }
   },
   "outputs": [],
   "source": [
    "train_exs = read_tweets_examples(train)\n",
    "val_exs = read_tweets_examples(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "surgical-virtue",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:36:40.366783Z",
     "start_time": "2021-08-12T04:36:40.364117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090/1523   train/valid  examples\n"
     ]
    }
   ],
   "source": [
    "print(repr(len(train_exs)) + \"/\" + repr(len(val_exs)) + \"  \" + \" train/valid  examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-stations",
   "metadata": {},
   "source": [
    "# DANClassifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "quality-armor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:36:40.381931Z",
     "start_time": "2021-08-12T04:36:40.371280Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN_pt(nn.Module):\n",
    "    def __init__(self, dict_length, embedding_size, weights):\n",
    "        super(RNN_pt, self).__init__()\n",
    "        # padding index turns off gradient for unknown tokens\n",
    "        self.word_emb = nn.Embedding.from_pretrained(form_input(word_embeddings.vectors), freeze=True, padding_idx=0)\n",
    "        self.rnn = nn.RNN(input_size=embedding_size, hidden_size=1, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.word_emb(x)\n",
    "        x = self.rnn(x.float())[1]\n",
    "    \n",
    "        return torch.squeeze(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "acquired-wallace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:36:40.390914Z",
     "start_time": "2021-08-12T04:36:40.384582Z"
    }
   },
   "outputs": [],
   "source": [
    "class DANClassifier(nn.Module):\n",
    "    def __init__(self, emb_size, hid, out, word_embeddings=None):\n",
    "        super(DANClassifier, self).__init__()\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.emb = nn.Embedding.from_pretrained(form_input(word_embeddings.vectors), padding_idx=0)\n",
    "        self.V = nn.Linear(emb_size, hid)\n",
    "        self.W = nn.Linear(hid, out)\n",
    "        self.g = nn.Sigmoid()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=0)\n",
    "        nn.init.xavier_uniform_(self.V.weight)\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Runs the neural network on the given data and returns log probabilities of the various classes.\n",
    "\n",
    "        :param x: a [inp]-sized tensor of input data\n",
    "        :return: an [out]-sized tensor of log probabilities. \n",
    "        \"\"\"\n",
    "        x = self.emb(x)\n",
    "        x = torch.mean(x, dim=0)\n",
    "        x = self.g(self.V(x))\n",
    "        x = self.W(x)\n",
    "        return self.log_softmax(x)\n",
    "\n",
    "    def predict(self, ex_words: List[str]) -> int:\n",
    "        word_indexer = self.word_embeddings.word_indexer\n",
    "        word_idx_list = []\n",
    "        for word in ex_words:\n",
    "            word_idx = word_indexer.index_of(word)\n",
    "            if word_idx == -1:\n",
    "                word_idx = 0\n",
    "            word_idx_list.append(word_idx)\n",
    "        x = torch.LongTensor(np.array(word_idx_list))\n",
    "        log_probs = self.forward(x)\n",
    "        y_pred = torch.argmax(log_probs)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_all(self, all_ex_words: List[List[str]]) -> List[int]:\n",
    "        \"\"\"      \n",
    "        :param all_ex_words: A list of all exs to do prediction on\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return [self.predict(ex_words) for ex_words in all_ex_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-slope",
   "metadata": {},
   "source": [
    "# Model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "liquid-projection",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:36:40.394711Z",
     "start_time": "2021-08-12T04:36:40.392671Z"
    }
   },
   "outputs": [],
   "source": [
    "def form_input(x) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Form the input to the neural network.\n",
    "\n",
    "    :param x: a [num_samples x inp] numpy array containing input data\n",
    "    :return: a [num_samples x inp] Tensor\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "senior-nurse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:36:40.403648Z",
     "start_time": "2021-08-12T04:36:40.395946Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_deep_averaging_network(train_exs: List[TweetsExample], val_exs: List[TweetsExample], word_embeddings: WordEmbeddings) -> DANClassifier:\n",
    "    \"\"\"\n",
    "    :param train_exs: training examples\n",
    "    :param val_exs: validation set, in case you wish to evaluate your model during training\n",
    "    :param word_embeddings: set of loaded word embeddings\n",
    "    :return: A trained DANClassifier model\n",
    "    \"\"\"\n",
    "    # DEFINE elements for DANClassifier init\n",
    "    emb_size = word_embeddings.get_embedding_length()\n",
    "    word_indexer = word_embeddings.word_indexer\n",
    "\n",
    "    hid_size = 500\n",
    "    num_classes = 2\n",
    "\n",
    "    # TRAINING\n",
    "    # set hyperparameters\n",
    "    num_epochs = 20\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # init model and optimizer\n",
    "    DAN = DANClassifier(emb_size, hid_size, num_classes, word_embeddings)\n",
    "    optimizer = optim.Adam(DAN.parameters(), lr=learning_rate)\n",
    "    # run training\n",
    "    for epoch in range(0, num_epochs):\n",
    "        ex_indices = [i for i in range(0, len(train_exs))]\n",
    "        random.shuffle(ex_indices)\n",
    "        total_loss = 0.0\n",
    "        for ex_idx in ex_indices:\n",
    "            word_idx_list = []\n",
    "            for word in train_exs[ex_idx].words:\n",
    "                word_idx = word_indexer.index_of(word)\n",
    "                if word_idx == -1:\n",
    "                    word_idx = 0\n",
    "                word_idx_list.append(word_idx)\n",
    "            x = torch.LongTensor(np.array(word_idx_list))\n",
    "            y = train_exs[ex_idx].label\n",
    "            # Build one-hot representation of y. Instead of the label 0 or 1, y_onehot is either [0, 1] or [1, 0]. This\n",
    "            # way we can take the dot product directly with a probability vector to get class probabilities.\n",
    "            y_onehot = torch.zeros(num_classes)\n",
    "            # scatter will write the value of 1 into the position of y_onehot given by y\n",
    "            y_onehot.scatter_(0, torch.from_numpy(np.asarray(y,dtype=np.int64)), 1)\n",
    "            # Zero out the gradients from the DAN object. *THIS IS VERY IMPORTANT TO DO BEFORE CALLING BACKWARD()*\n",
    "            DAN.zero_grad()\n",
    "            log_probs = DAN.forward(x)\n",
    "            # Can also use built-in NLLLoss as a shortcut here but we're being explicit here\n",
    "            loss = torch.neg(log_probs).dot(y_onehot)\n",
    "            total_loss += loss\n",
    "            # Computes the gradient and takes the optimizer step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Total loss on epoch %i: %f\" % (epoch, total_loss))\n",
    "    return DAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "overhead-freight",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:37:55.453000Z",
     "start_time": "2021-08-12T04:36:40.405806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss on epoch 0: 3289.665283\n",
      "Total loss on epoch 1: 3162.134766\n",
      "Total loss on epoch 2: 3059.186279\n",
      "Total loss on epoch 3: 3002.518555\n",
      "Total loss on epoch 4: 2961.735596\n",
      "Total loss on epoch 5: 2897.091309\n",
      "Total loss on epoch 6: 2867.951416\n",
      "Total loss on epoch 7: 2838.942383\n",
      "Total loss on epoch 8: 2809.219971\n",
      "Total loss on epoch 9: 2789.738281\n",
      "Total loss on epoch 10: 2759.521240\n",
      "Total loss on epoch 11: 2754.818359\n",
      "Total loss on epoch 12: 2729.041748\n",
      "Total loss on epoch 13: 2703.055908\n",
      "Total loss on epoch 14: 2677.321045\n",
      "Total loss on epoch 15: 2653.559326\n",
      "Total loss on epoch 16: 2636.763428\n",
      "Total loss on epoch 17: 2616.576904\n",
      "Total loss on epoch 18: 2587.018799\n",
      "Total loss on epoch 19: 2564.921631\n"
     ]
    }
   ],
   "source": [
    "model = train_deep_averaging_network(train_exs, val_exs, word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-header",
   "metadata": {},
   "source": [
    "# Model evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "proud-integration",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:37:55.458202Z",
     "start_time": "2021-08-12T04:37:55.455542Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(classifier, exs):\n",
    "    \"\"\"\n",
    "    Evaluates a given classifier on the given examples\n",
    "    :param classifier: classifier to evaluate\n",
    "    :param exs: the list of TweetsExamples to evaluate on\n",
    "    :return: None (but prints output)\n",
    "    \"\"\"\n",
    "    return print_evaluation([ex.label for ex in exs], classifier.predict_all([ex.words for ex in exs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "tribal-marine",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:37:55.470296Z",
     "start_time": "2021-08-12T04:37:55.462900Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_evaluation(golds: List[int], predictions: List[int]):\n",
    "    \"\"\"\n",
    "    Prints evaluation statistics comparing golds and predictions, each of which is a sequence of 0/1 labels.\n",
    "    Prints accuracy as well as precision/recall/F1 of the positive class, which can sometimes be informative if either\n",
    "    the golds or predictions are highly biased.\n",
    "\n",
    "    :param golds: gold labels\n",
    "    :param predictions: pred labels\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_pos_correct = 0\n",
    "    num_pred = 0\n",
    "    num_gold = 0\n",
    "    num_total = 0\n",
    "    if len(golds) != len(predictions):\n",
    "        raise Exception(\"Mismatched gold/pred lengths: %i / %i\" % (len(golds), len(predictions)))\n",
    "    for idx in range(0, len(golds)):\n",
    "        gold = golds[idx]\n",
    "        prediction = predictions[idx]\n",
    "        if prediction == gold:\n",
    "            num_correct += 1\n",
    "        if prediction == 1:\n",
    "            num_pred += 1\n",
    "        if gold == 1:\n",
    "            num_gold += 1\n",
    "        if prediction == 1 and gold == 1:\n",
    "            num_pos_correct += 1\n",
    "        num_total += 1\n",
    "    acc = float(num_correct) / num_total\n",
    "    output_str = \"Accuracy: %i / %i = %f\" % (num_correct, num_total, acc)\n",
    "    prec = float(num_pos_correct) / num_pred if num_pred > 0 else 0.0\n",
    "    rec = float(num_pos_correct) / num_gold if num_gold > 0 else 0.0\n",
    "    f1 = 2 * prec * rec / (prec + rec) if prec > 0 and rec > 0 else 0.0\n",
    "    output_str += \";\\nPrecision (fraction of predicted positives that are correct): %i / %i = %f\" % (num_pos_correct, num_pred, prec)\n",
    "    output_str += \";\\nRecall (fraction of true positives predicted correctly): %i / %i = %f\" % (num_pos_correct, num_gold, rec)\n",
    "    output_str += \";\\nF1 (harmonic mean of precision and recall): %f;\\n\" % f1\n",
    "    print(output_str)\n",
    "    return acc, f1, output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "municipal-tracker",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T04:37:56.311270Z",
     "start_time": "2021-08-12T04:37:55.471863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Train Accuracy=====\n",
      "Accuracy: 4986 / 6090 = 0.818719;\n",
      "Precision (fraction of predicted positives that are correct): 1794 / 2067 = 0.867925;\n",
      "Recall (fraction of true positives predicted correctly): 1794 / 2625 = 0.683429;\n",
      "F1 (harmonic mean of precision and recall): 0.764706;\n",
      "\n",
      "=====Val Accuracy=====\n",
      "Accuracy: 1223 / 1523 = 0.803020;\n",
      "Precision (fraction of predicted positives that are correct): 433 / 520 = 0.832692;\n",
      "Recall (fraction of true positives predicted correctly): 433 / 646 = 0.670279;\n",
      "F1 (harmonic mean of precision and recall): 0.742710;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=====Train Accuracy=====\")\n",
    "train_acc, train_f1, train_out = evaluate(model, train_exs)\n",
    "print(\"=====Val Accuracy=====\")\n",
    "val_acc, val_f1, val_out = evaluate(model, val_exs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-illinois",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "277px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
